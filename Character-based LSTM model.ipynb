{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "gather": {
     "logged": 1607449616267
    },
    "id": "rEceehyQfXoV",
    "outputId": "74861111-a4a1-476a-d85c-d45ae585d660"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<module 'tensorflow._api.v2.version' from '/usr/local/lib/python3.6/dist-packages/tensorflow/_api/v2/version/__init__.py'>\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "gather": {
     "logged": 1607449617014
    },
    "id": "C5zcgNLUfXod",
    "outputId": "de0d83f5-aacf-4e06-b745-d907a721c53e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 1155335 characters\n",
      "46 unique characters\n"
     ]
    }
   ],
   "source": [
    "# Read, then decode for py2 compat.\n",
    "text = open('/content/sample_data/praise-poems_dataset.txt', 'rb').read().decode(encoding='utf-8')\n",
    "# length of text is the number of characters in it\n",
    "print ('Length of text: {} characters'.format(len(text)))\n",
    "\n",
    "# remove some exteranous chars \n",
    "execluded = '!()*-.1:=[]«»;؛−,،~?؟#\\u200f\\ufeff'\n",
    "out = \"\"\n",
    "for char in text:\n",
    "  if char not in execluded:\n",
    "    out += char\n",
    "text = out\n",
    "text = text.replace(\"\\t\\t\\t\", \"\\t\")\n",
    "text = text.replace(\"\\r\\r\\n\", \"\\n\")\n",
    "text = text.replace(\"\\r\\n\",\"\\n\")\n",
    "text = text.replace(\"\\t\\n\", \"\\n\")\n",
    "vocab = sorted(set(text))\n",
    "print ('{} unique characters'.format(len(vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "gather": {
     "logged": 1607449617147
    },
    "id": "Ulqq5ciffXoe",
    "outputId": "a43e7ade-a6c9-4de1-c8fd-d2fd347d4bce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "عَبَّاسُ يا خَيْرَ المُلُوكِ عَدَالَةً\n",
      "وَأَجَلَّ مَنْ نَطَقَ امْرُؤٌ بِثَنَائِهِ\n",
      "أَوْلَيْتَنِي مِنْكَ الرِّضَا وجَلَوْتَ لِي\n",
      "وَجْهاً قَرَأْتُ البِشْرَ في أَثْنَائِهِ\n",
      "فاسْلَمْ لِمُلْكٍ أَنْتَ بَدْرُ سَ\n"
     ]
    }
   ],
   "source": [
    "# Take a look at the first 250 characters in text\n",
    "print(text[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1607449617338
    },
    "id": "1nlLbhtJfXof"
   },
   "outputs": [],
   "source": [
    "# Creating a mapping from unique characters to indices\n",
    "char2idx = {u:i for i, u in enumerate(vocab)}\n",
    "idx2char = np.array(vocab)\n",
    "text_as_int = np.array([char2idx[c] for c in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "gather": {
     "logged": 1607449617383
    },
    "id": "4LjoaKCAfXof",
    "outputId": "6a6ab2a6-c819-4eec-c413-fdcb887f1c67"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  '\\n':   0,\n",
      "  ' ' :   1,\n",
      "  'ء' :   2,\n",
      "  'آ' :   3,\n",
      "  'أ' :   4,\n",
      "  'ؤ' :   5,\n",
      "  'إ' :   6,\n",
      "  'ئ' :   7,\n",
      "  'ا' :   8,\n",
      "  'ب' :   9,\n",
      "  'ة' :  10,\n",
      "  'ت' :  11,\n",
      "  'ث' :  12,\n",
      "  'ج' :  13,\n",
      "  'ح' :  14,\n",
      "  'خ' :  15,\n",
      "  'د' :  16,\n",
      "  'ذ' :  17,\n",
      "  'ر' :  18,\n",
      "  'ز' :  19,\n",
      "  'س' :  20,\n",
      "  'ش' :  21,\n",
      "  'ص' :  22,\n",
      "  'ض' :  23,\n",
      "  'ط' :  24,\n",
      "  'ظ' :  25,\n",
      "  'ع' :  26,\n",
      "  'غ' :  27,\n",
      "  'ف' :  28,\n",
      "  'ق' :  29,\n",
      "  'ك' :  30,\n",
      "  'ل' :  31,\n",
      "  'م' :  32,\n",
      "  'ن' :  33,\n",
      "  'ه' :  34,\n",
      "  'و' :  35,\n",
      "  'ى' :  36,\n",
      "  'ي' :  37,\n",
      "  'ً' :  38,\n",
      "  'ٌ' :  39,\n",
      "  'ٍ' :  40,\n",
      "  'َ' :  41,\n",
      "  'ُ' :  42,\n",
      "  'ِ' :  43,\n",
      "  'ّ' :  44,\n",
      "  'ْ' :  45,\n",
      "  ...\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print('{')\n",
    "for char,_ in zip(char2idx, range(47)):\n",
    "    print('  {:4s}: {:3d},'.format(repr(char), char2idx[char]))\n",
    "print('  ...\\n}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "gather": {
     "logged": 1607449617488
    },
    "id": "e8FYdihFfXof",
    "outputId": "4d75c3ad-7866-4496-e76e-119519fddb99"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'عَبَّاسُ يا خَيْرَ المُلُوكِ' ---- characters mapped to int ---- > [26 41  9 44 41  8 20 42  1 37  8  1 15 41 37 45 18 41  1  8 31 32 42 31\n",
      " 42 35 30 43]\n"
     ]
    }
   ],
   "source": [
    "# Show how the first 13 characters from the text are mapped to integers\n",
    "print ('{} ---- characters mapped to int ---- > {}'.format(repr(text[:28]), text_as_int[:28]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VT3AT9SB2RD7"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "gather": {
     "logged": 1607449617617
    },
    "id": "H2so1F2zfXog",
    "outputId": "033a2864-ca9c-4fdb-ec69-b326be674576"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ع\n",
      "َ\n",
      "ب\n",
      "ّ\n",
      "َ\n"
     ]
    }
   ],
   "source": [
    "# The maximum length sentence we want for a single input in characters\n",
    "seq_length = 200\n",
    "examples_per_epoch = len(text)//seq_length\n",
    "\n",
    "# Create training examples / targets\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
    "\n",
    "for i in char_dataset.take(5):\n",
    "  print(idx2char[i.numpy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VjA1-4_X2p7S",
    "outputId": "611ccdcd-5cf2-472b-9a4c-4dc9a9c47c85"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[26 41  9 ... 18 37  8]\n"
     ]
    }
   ],
   "source": [
    "print(text_as_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "gather": {
     "logged": 1607449617755
    },
    "id": "0eysV0HEfXog",
    "outputId": "5779eff3-a3ae-4332-ece8-503630073cea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'عَبَّاسُ يا خَيْرَ المُلُوكِ عَدَالَةً\\nوَأَجَلَّ مَنْ نَطَقَ امْرُؤٌ بِثَنَائِهِ\\nأَوْلَيْتَنِي مِنْكَ الرِّضَا وجَلَوْتَ لِي\\nوَجْهاً قَرَأْتُ البِشْرَ في أَثْنَائِهِ\\nفاسْلَمْ لِمُلْكٍ أَنْتَ بَدْرُ سَر'\n",
      "'ِيرِهِ\\nوَعِمَادُ قُوَّتِهِ ونَصْرُ لِوائِهِ\\nيأَيُّها الصَّادِي إِلى نَيْلِ الْمُنَى\\nرِدْ بَحْرَ سُدَّتِهِ تَفُزْ بِوَلائِهِ\\nهُوَ ذَلِكَ الْمَلِكُ الَّذِي وَرِث الْعُلا\\nعَنْ نَفْسِهِ شَرَفا وعَن آبائِهِ'\n",
      "'\\nالْعَدْلُ مِنْ أَخْلاقِهِ والْعِلْمُ مِنْ\\nأَوْصافِهِ والْحِلْمُ مِنْ أَسْمَائِهِ\\nلا غَرْوَ أَنْ جَمَعَ المَحَامِدَ يافِعاً\\nوَسَمَا بِهِمَّتِهِ عَلَى نُظَرائِهِ\\nفالْعَينُ وَهْيَ صَغِيرَةٌ في حَجْمِها\\nت'\n",
      "'َسَعُ الفَضَاءَ بِأَرْضِهِ وسَمائِهِ\\n\\nوإني حين تشتجر العوالي\\nأعيد الرمح في أثر الجراح\\nشديد البأس ليس بذي عياء\\nولكني أبوء إلى الفلاح\\nسألبس ثوبها وأذب عنها\\nبأطراف العوالي والصفاحف\\nما يبقى لعترته ذليل\\nفتم'\n",
      "'نعه من القدر المتاح\\nوأجمل من حياة الذل موت\\nوبعض العار لا يمحوه ماح\\n\\nوَلَو بَرَزَ الزَمانُ إِلَيَّ شَخصاً\\nلَخَضَّبَ شَعرَ مَفرِقِهِ حُسامي\\nوَما بَلَغَت مَشيئتَها اللَيالي\\nوَلا سارَت وَفي يَدِها زِمامي\\nإ'\n"
     ]
    }
   ],
   "source": [
    "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "\n",
    "for item in sequences.take(5):\n",
    "  print(repr(''.join(idx2char[item.numpy()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1607449618051
    },
    "id": "jBbA18BKfXoh"
   },
   "outputs": [],
   "source": [
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "dataset = sequences.map(split_input_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "gather": {
     "logged": 1607449618253
    },
    "id": "J_7q-l2KfXoh",
    "outputId": "30dcd934-3e97-4651-cdfe-e4a266901065"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data:  'عَبَّاسُ يا خَيْرَ المُلُوكِ عَدَالَةً\\nوَأَجَلَّ مَنْ نَطَقَ امْرُؤٌ بِثَنَائِهِ\\nأَوْلَيْتَنِي مِنْكَ الرِّضَا وجَلَوْتَ لِي\\nوَجْهاً قَرَأْتُ البِشْرَ في أَثْنَائِهِ\\nفاسْلَمْ لِمُلْكٍ أَنْتَ بَدْرُ سَ'\n",
      "Target data: 'َبَّاسُ يا خَيْرَ المُلُوكِ عَدَالَةً\\nوَأَجَلَّ مَنْ نَطَقَ امْرُؤٌ بِثَنَائِهِ\\nأَوْلَيْتَنِي مِنْكَ الرِّضَا وجَلَوْتَ لِي\\nوَجْهاً قَرَأْتُ البِشْرَ في أَثْنَائِهِ\\nفاسْلَمْ لِمُلْكٍ أَنْتَ بَدْرُ سَر'\n"
     ]
    }
   ],
   "source": [
    "#Print the first examples input and target values:\n",
    "for input_example, target_example in  dataset.take(1):\n",
    "  print ('Input data: ', repr(''.join(idx2char[input_example.numpy()])))\n",
    "  print ('Target data:', repr(''.join(idx2char[target_example.numpy()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "gather": {
     "logged": 1607449618388
    },
    "id": "MldV7j9RfXoh",
    "outputId": "6c630da7-6ebb-499f-f181-797303cf16e2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step    0\n",
      "  input: 26 ('ع')\n",
      "  expected output: 41 ('َ')\n",
      "Step    1\n",
      "  input: 41 ('َ')\n",
      "  expected output: 9 ('ب')\n",
      "Step    2\n",
      "  input: 9 ('ب')\n",
      "  expected output: 44 ('ّ')\n",
      "Step    3\n",
      "  input: 44 ('ّ')\n",
      "  expected output: 41 ('َ')\n",
      "Step    4\n",
      "  input: 41 ('َ')\n",
      "  expected output: 8 ('ا')\n",
      "Step    5\n",
      "  input: 8 ('ا')\n",
      "  expected output: 20 ('س')\n",
      "Step    6\n",
      "  input: 20 ('س')\n",
      "  expected output: 42 ('ُ')\n",
      "Step    7\n",
      "  input: 42 ('ُ')\n",
      "  expected output: 1 (' ')\n"
     ]
    }
   ],
   "source": [
    "for i, (input_idx, target_idx) in enumerate(zip(input_example[:8], target_example[:8])):\n",
    "    print(\"Step {:4d}\".format(i))\n",
    "    print(\"  input: {} ({:s})\".format(input_idx, repr(idx2char[input_idx])))\n",
    "    print(\"  expected output: {} ({:s})\".format(target_idx, repr(idx2char[target_idx])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "gather": {
     "logged": 1607449618447
    },
    "id": "8x-MYgM_fXoh",
    "outputId": "f1c34e4b-65c5-453e-aa0f-beacfe285aa9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((128, 200), (128, 200)), types: (tf.int64, tf.int64)>"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create training batches\n",
    "# Batch size\n",
    "BATCH_SIZE = 128 \n",
    "steps_per_epoch = examples_per_epoch//BATCH_SIZE\n",
    "\n",
    "# Buffer size to shuffle the dataset\n",
    "# (TF data is designed to work with possibly infinite sequences,\n",
    "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
    "# it maintains a buffer in which it shuffles elements).\n",
    "BUFFER_SIZE = 10000 \n",
    "\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1607449618636
    },
    "id": "CKtFInTpfXoi"
   },
   "outputs": [],
   "source": [
    "#Build The Model\n",
    "# Length of the vocabulary in chars\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# The embedding dimension\n",
    "embedding_dim = 256\n",
    "\n",
    "# Number of RNN units\n",
    "rnn_units = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1607449618772
    },
    "id": "histX2EvfXoi"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
    "#function to build the model.\n",
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "  model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
    "                              batch_input_shape=[batch_size, None]),\n",
    "    tf.keras.layers.LSTM(rnn_units,\n",
    "        return_sequences=True,\n",
    "        recurrent_initializer='glorot_uniform',\n",
    "        stateful=True),\n",
    "    tf.keras.layers.Dense(vocab_size)\n",
    "  ])\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1607449618924
    },
    "id": "kgLj40BofXoi"
   },
   "outputs": [],
   "source": [
    "model = build_model(\n",
    "  vocab_size = len(vocab),\n",
    "  embedding_dim=embedding_dim,\n",
    "  rnn_units=rnn_units,\n",
    "  batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "gather": {
     "logged": 1607449622727
    },
    "id": "L9Sk05gCfXoj",
    "outputId": "fc40c6e1-641b-48ba-e252-559c1844159b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 200, 46) # (batch_size, sequence_length, vocab_size)\n"
     ]
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "  example_batch_predictions = model(input_example_batch)\n",
    "  print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "gather": {
     "logged": 1607449622981
    },
    "id": "nUdwq0UofXoj",
    "outputId": "9b8844ec-5dcb-46fa-9307-ca4d162d1946"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (128, None, 256)          11776     \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (128, None, 1024)         5246976   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (128, None, 46)           47150     \n",
      "=================================================================\n",
      "Total params: 5,305,902\n",
      "Trainable params: 5,305,902\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1607449623068
    },
    "id": "Ha9zKFy6fXoj"
   },
   "outputs": [],
   "source": [
    "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
    "sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "gather": {
     "logged": 1607449623302
    },
    "id": "hXrecS8nfXoj",
    "outputId": "b597ac2d-adb2-44e2-bb4f-8342b1575c25"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([16, 16, 20, 34, 31, 34, 19, 17, 28,  2, 45, 18, 20, 19,  8, 37, 41,\n",
       "       10, 45,  7, 30, 31, 28, 33, 26,  1, 30, 19, 43, 19, 34, 17, 14, 21,\n",
       "       18, 15, 10,  2, 38, 14, 10,  7, 12, 34, 16, 44,  4, 10, 15,  0,  3,\n",
       "        3, 27, 42,  0,  0, 41, 21, 31, 39, 45, 36, 39, 21, 19, 11,  9, 39,\n",
       "        9,  7, 22,  2, 29,  2, 30, 32, 39,  4, 22, 40, 45, 41,  2, 13, 27,\n",
       "       10, 36, 30, 35, 41, 13, 21, 36, 28, 44, 37, 12, 35,  9, 17, 24, 44,\n",
       "        7,  4,  8, 38,  3, 23, 21, 29,  0, 28, 27, 13,  5, 34, 39, 17, 15,\n",
       "        9,  0,  7, 24, 42,  6, 10, 37, 31, 40, 40, 34, 40, 40, 39, 24, 27,\n",
       "        2, 25, 31,  2, 42, 19,  6, 45,  0, 44, 23, 21, 12, 20, 43,  5, 36,\n",
       "        7, 21, 40, 40, 11,  5, 27,  1, 42, 43, 12,  0, 24, 34,  9, 42, 10,\n",
       "       42, 44, 41, 29, 26,  3, 11, 26, 42, 19, 11, 28, 14,  7, 41,  8, 14,\n",
       "       39, 38, 35,  5, 35, 45, 39,  9,  5, 36, 35, 43, 39])"
      ]
     },
     "execution_count": 20,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This gives us, at each timestep, a prediction of the next character index:\n",
    "print(len(sampled_indices))\n",
    "sampled_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "gather": {
     "logged": 1607449623353
    },
    "id": "tYFaB4pQfXok",
    "outputId": "4074d6c9-de29-4840-9b2d-0512e539e4e4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: \n",
      " ' وأبقاكَ آخرَ الدهر عصرا\\nفي سرورٍ يُريك شهرك يوماً\\nوحبورٍ يريك عامك شهرا\\nقلت لما بدا الهلالُ ضئيلاً\\nقد كستْه سُرى ثلاثين ضُمرا\\nعجباً للهلال كيف استهلّو\\nهُ هلالاً هلَّا استهلوه بدرا\\nكان لما بدا وأنت أم'\n",
      "\n",
      "Next Char Predictions: \n",
      " 'ددسهلهزذفءْرسزايَةْئكلفنع كزِزهذحشرخةءًحةئثهدّأةخ\\nآآغُ\\n\\nَشلٌْىٌشزتبٌبئصءقءكمٌأصٍَْءجغةىكوَجشىفّيثوبذطّئأاًآضشق\\nفغجؤهٌذخب\\nئطُإةيلٍٍهٌٍٍطغءظلءُزإْ\\nّضشثسِؤىئشٍٍتؤغ ُِث\\nطهبُةَُّقعآتعُزتفحئَاحًٌوؤوٌْبؤىوٌِ'\n"
     ]
    }
   ],
   "source": [
    "#Decode these to see the text predicted by this untrained model:\n",
    "print(\"Input: \\n\", repr(\"\".join(idx2char[input_example_batch[0].numpy()])))\n",
    "print()\n",
    "print(\"Next Char Predictions: \\n\", repr(\"\".join(idx2char[sampled_indices ])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bPkTZzH_fXok",
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "Attach an optimizer, and a loss function\n",
    "The standard tf.keras.losses.sparse_softmax_crossentropy loss function works in this case because it is applied across the last dimension of the predictions.\n",
    "Because our model returns logits, we need to set the from_logits flag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "gather": {
     "logged": 1607449623501
    },
    "id": "Q30LusKIfXok",
    "outputId": "1f5bec2b-263c-4f38-dc91-46e4378c497f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction shape:  (128, 200, 46)  # (batch_size, sequence_length, vocab_size)\n",
      "scalar_loss:       3.8289192\n"
     ]
    }
   ],
   "source": [
    "def loss(labels, logits):\n",
    "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "\n",
    "example_batch_loss  = loss(target_example_batch, example_batch_predictions)\n",
    "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
    "print(\"scalar_loss:      \", example_batch_loss.numpy().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1607449623783
    },
    "id": "ZX4x6y-YfXok"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=loss)\n",
    "    #optimizer = tf.optimizers.Adam(),loss = loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1607449623831
    },
    "id": "iBbYpjP-fXol"
   },
   "outputs": [],
   "source": [
    "# Directory where the checkpoints will be saved\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1607449623966
    },
    "id": "V4Ci6_HAfXol"
   },
   "outputs": [],
   "source": [
    "EPOCHS=300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "gather": {
     "logged": 1607030989986
    },
    "id": "4zhEoFjWfXol",
    "outputId": "7cfff01a-ece4-4d43-a33d-7a65fc7a1e0f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "43/43 [==============================] - 23s 539ms/step - loss: 3.2609\n",
      "Epoch 2/300\n",
      "43/43 [==============================] - 23s 535ms/step - loss: 2.7083\n",
      "Epoch 3/300\n",
      "43/43 [==============================] - 23s 533ms/step - loss: 2.4770\n",
      "Epoch 4/300\n",
      "43/43 [==============================] - 23s 534ms/step - loss: 2.3536\n",
      "Epoch 5/300\n",
      "43/43 [==============================] - 23s 534ms/step - loss: 2.2473\n",
      "Epoch 6/300\n",
      "43/43 [==============================] - 23s 534ms/step - loss: 2.1542\n",
      "Epoch 7/300\n",
      "43/43 [==============================] - 23s 534ms/step - loss: 2.0875\n",
      "Epoch 8/300\n",
      "43/43 [==============================] - 23s 534ms/step - loss: 2.0310\n",
      "Epoch 9/300\n",
      "43/43 [==============================] - 23s 535ms/step - loss: 1.9811\n",
      "Epoch 10/300\n",
      "43/43 [==============================] - 23s 535ms/step - loss: 1.9392\n",
      "Epoch 11/300\n",
      "43/43 [==============================] - 23s 534ms/step - loss: 1.9026\n",
      "Epoch 12/300\n",
      "43/43 [==============================] - 23s 535ms/step - loss: 1.8659\n",
      "Epoch 13/300\n",
      "43/43 [==============================] - 23s 534ms/step - loss: 1.8324\n",
      "Epoch 14/300\n",
      "43/43 [==============================] - 23s 533ms/step - loss: 1.7990\n",
      "Epoch 15/300\n",
      "43/43 [==============================] - 23s 534ms/step - loss: 1.7691\n",
      "Epoch 16/300\n",
      "43/43 [==============================] - 23s 535ms/step - loss: 1.7372\n",
      "Epoch 17/300\n",
      "43/43 [==============================] - 23s 534ms/step - loss: 1.7077\n",
      "Epoch 18/300\n",
      "43/43 [==============================] - 23s 534ms/step - loss: 1.6766\n",
      "Epoch 19/300\n",
      "43/43 [==============================] - 23s 534ms/step - loss: 1.6446\n",
      "Epoch 20/300\n",
      "43/43 [==============================] - 23s 534ms/step - loss: 1.6113\n",
      "Epoch 21/300\n",
      "43/43 [==============================] - 23s 534ms/step - loss: 1.5798\n",
      "Epoch 22/300\n",
      "43/43 [==============================] - 23s 534ms/step - loss: 1.5441\n",
      "Epoch 23/300\n",
      "43/43 [==============================] - 23s 534ms/step - loss: 1.5100\n",
      "Epoch 24/300\n",
      "43/43 [==============================] - 23s 534ms/step - loss: 1.4744\n",
      "Epoch 25/300\n",
      "43/43 [==============================] - 23s 535ms/step - loss: 1.4381\n",
      "Epoch 26/300\n",
      "43/43 [==============================] - 23s 535ms/step - loss: 1.3991\n",
      "Epoch 27/300\n",
      "43/43 [==============================] - 23s 536ms/step - loss: 1.3610\n",
      "Epoch 28/300\n",
      "43/43 [==============================] - 23s 537ms/step - loss: 1.3210\n",
      "Epoch 29/300\n",
      "43/43 [==============================] - 23s 537ms/step - loss: 1.2796\n",
      "Epoch 30/300\n",
      "43/43 [==============================] - 23s 536ms/step - loss: 1.2375\n",
      "Epoch 31/300\n",
      "43/43 [==============================] - 23s 536ms/step - loss: 1.1944\n",
      "Epoch 32/300\n",
      "43/43 [==============================] - 23s 537ms/step - loss: 1.1529\n",
      "Epoch 33/300\n",
      "43/43 [==============================] - 23s 536ms/step - loss: 1.1129\n",
      "Epoch 34/300\n",
      "43/43 [==============================] - 23s 536ms/step - loss: 1.0686\n",
      "Epoch 35/300\n",
      "43/43 [==============================] - 23s 536ms/step - loss: 1.0297\n",
      "Epoch 36/300\n",
      "43/43 [==============================] - 23s 537ms/step - loss: 0.9880\n",
      "Epoch 37/300\n",
      "43/43 [==============================] - 23s 534ms/step - loss: 0.9495\n",
      "Epoch 38/300\n",
      "43/43 [==============================] - 23s 532ms/step - loss: 0.9152\n",
      "Epoch 39/300\n",
      "43/43 [==============================] - 23s 532ms/step - loss: 0.8810\n",
      "Epoch 40/300\n",
      "43/43 [==============================] - 23s 531ms/step - loss: 0.8472\n",
      "Epoch 41/300\n",
      "43/43 [==============================] - 23s 531ms/step - loss: 0.8137\n",
      "Epoch 42/300\n",
      "43/43 [==============================] - 23s 532ms/step - loss: 0.7829\n",
      "Epoch 43/300\n",
      "43/43 [==============================] - 23s 532ms/step - loss: 0.7548\n",
      "Epoch 44/300\n",
      "43/43 [==============================] - 23s 533ms/step - loss: 0.7280\n",
      "Epoch 45/300\n",
      "43/43 [==============================] - 23s 532ms/step - loss: 0.7008\n",
      "Epoch 46/300\n",
      "43/43 [==============================] - 23s 533ms/step - loss: 0.6775\n",
      "Epoch 47/300\n",
      "43/43 [==============================] - 23s 532ms/step - loss: 0.6562\n",
      "Epoch 48/300\n",
      "43/43 [==============================] - 23s 533ms/step - loss: 0.6355\n",
      "Epoch 49/300\n",
      "43/43 [==============================] - 23s 532ms/step - loss: 0.6163\n",
      "Epoch 50/300\n",
      "43/43 [==============================] - 23s 532ms/step - loss: 0.5952\n",
      "Epoch 51/300\n",
      "43/43 [==============================] - 23s 533ms/step - loss: 0.5753\n",
      "Epoch 52/300\n",
      "43/43 [==============================] - 23s 532ms/step - loss: 0.5589\n",
      "Epoch 53/300\n",
      "43/43 [==============================] - 23s 533ms/step - loss: 0.5426\n",
      "Epoch 54/300\n",
      "43/43 [==============================] - 23s 534ms/step - loss: 0.5290\n",
      "Epoch 55/300\n",
      "43/43 [==============================] - 23s 535ms/step - loss: 0.5151\n",
      "Epoch 56/300\n",
      "43/43 [==============================] - 23s 535ms/step - loss: 0.5003\n",
      "Epoch 57/300\n",
      "43/43 [==============================] - 23s 533ms/step - loss: 0.4892\n",
      "Epoch 58/300\n",
      "43/43 [==============================] - 23s 533ms/step - loss: 0.4755\n",
      "Epoch 59/300\n",
      "43/43 [==============================] - 23s 532ms/step - loss: 0.4652\n",
      "Epoch 60/300\n",
      "43/43 [==============================] - 23s 534ms/step - loss: 0.4541\n",
      "Epoch 61/300\n",
      "43/43 [==============================] - 23s 532ms/step - loss: 0.4461\n",
      "Epoch 62/300\n",
      "43/43 [==============================] - 23s 532ms/step - loss: 0.4359\n",
      "Epoch 63/300\n",
      "43/43 [==============================] - 23s 531ms/step - loss: 0.4292\n",
      "Epoch 64/300\n",
      "43/43 [==============================] - 23s 532ms/step - loss: 0.4191\n",
      "Epoch 65/300\n",
      "43/43 [==============================] - 23s 531ms/step - loss: 0.4156\n",
      "Epoch 66/300\n",
      "43/43 [==============================] - 23s 532ms/step - loss: 0.4081\n",
      "Epoch 67/300\n",
      "43/43 [==============================] - 23s 530ms/step - loss: 0.3975\n",
      "Epoch 68/300\n",
      "43/43 [==============================] - 23s 531ms/step - loss: 0.3881\n",
      "Epoch 69/300\n",
      "43/43 [==============================] - 23s 531ms/step - loss: 0.3820\n",
      "Epoch 70/300\n",
      "43/43 [==============================] - 23s 532ms/step - loss: 0.3778\n",
      "Epoch 71/300\n",
      "43/43 [==============================] - 23s 533ms/step - loss: 0.3750\n",
      "Epoch 72/300\n",
      "43/43 [==============================] - 23s 533ms/step - loss: 0.3716\n",
      "Epoch 73/300\n",
      "43/43 [==============================] - 23s 532ms/step - loss: 0.3666\n",
      "Epoch 74/300\n",
      "43/43 [==============================] - 23s 531ms/step - loss: 0.3611\n",
      "Epoch 75/300\n",
      "43/43 [==============================] - 23s 531ms/step - loss: 0.3549\n",
      "Epoch 76/300\n",
      "43/43 [==============================] - 23s 531ms/step - loss: 0.3499\n",
      "Epoch 77/300\n",
      "43/43 [==============================] - 23s 529ms/step - loss: 0.3447\n",
      "Epoch 78/300\n",
      "43/43 [==============================] - 23s 530ms/step - loss: 0.3428\n",
      "Epoch 79/300\n",
      "43/43 [==============================] - 23s 531ms/step - loss: 0.3396\n",
      "Epoch 80/300\n",
      "43/43 [==============================] - 23s 529ms/step - loss: 0.3366\n",
      "Epoch 81/300\n",
      "43/43 [==============================] - 23s 530ms/step - loss: 0.3347\n",
      "Epoch 82/300\n",
      "43/43 [==============================] - 23s 528ms/step - loss: 0.3308\n",
      "Epoch 83/300\n",
      "43/43 [==============================] - 23s 528ms/step - loss: 0.3244\n",
      "Epoch 84/300\n",
      "43/43 [==============================] - 23s 531ms/step - loss: 0.3214\n",
      "Epoch 85/300\n",
      "43/43 [==============================] - 23s 530ms/step - loss: 0.3207\n",
      "Epoch 86/300\n",
      "43/43 [==============================] - 23s 529ms/step - loss: 0.3200\n",
      "Epoch 87/300\n",
      "43/43 [==============================] - 23s 529ms/step - loss: 0.3214\n",
      "Epoch 88/300\n",
      "43/43 [==============================] - 23s 531ms/step - loss: 0.3181\n",
      "Epoch 89/300\n",
      "43/43 [==============================] - 23s 529ms/step - loss: 0.3161\n",
      "Epoch 90/300\n",
      "43/43 [==============================] - 23s 530ms/step - loss: 0.3135\n",
      "Epoch 91/300\n",
      "43/43 [==============================] - 23s 527ms/step - loss: 0.3088\n",
      "Epoch 92/300\n",
      "43/43 [==============================] - 23s 530ms/step - loss: 0.3072\n",
      "Epoch 93/300\n",
      "43/43 [==============================] - 23s 529ms/step - loss: 0.3042\n",
      "Epoch 94/300\n",
      "43/43 [==============================] - 23s 530ms/step - loss: 0.3037\n",
      "Epoch 95/300\n",
      "43/43 [==============================] - 23s 530ms/step - loss: 0.3087\n",
      "Epoch 96/300\n",
      "43/43 [==============================] - 23s 528ms/step - loss: 0.3111\n",
      "Epoch 97/300\n",
      "43/43 [==============================] - 23s 530ms/step - loss: 0.3106\n",
      "Epoch 98/300\n",
      "43/43 [==============================] - 23s 530ms/step - loss: 0.3045\n",
      "Epoch 99/300\n",
      "43/43 [==============================] - 23s 529ms/step - loss: 0.3013\n",
      "Epoch 100/300\n",
      "43/43 [==============================] - 23s 530ms/step - loss: 0.3000\n",
      "Epoch 101/300\n",
      "43/43 [==============================] - 23s 528ms/step - loss: 0.2982\n",
      "Epoch 102/300\n",
      "43/43 [==============================] - 23s 530ms/step - loss: 0.2955\n",
      "Epoch 103/300\n",
      "43/43 [==============================] - 23s 529ms/step - loss: 0.2912\n",
      "Epoch 104/300\n",
      "43/43 [==============================] - 23s 530ms/step - loss: 0.2855\n",
      "Epoch 105/300\n",
      "43/43 [==============================] - 23s 530ms/step - loss: 0.2842\n",
      "Epoch 106/300\n",
      "43/43 [==============================] - 23s 532ms/step - loss: 0.2846\n",
      "Epoch 107/300\n",
      "43/43 [==============================] - 23s 529ms/step - loss: 0.2846\n",
      "Epoch 108/300\n",
      "43/43 [==============================] - 23s 529ms/step - loss: 0.2838\n",
      "Epoch 109/300\n",
      "43/43 [==============================] - 23s 528ms/step - loss: 0.2818\n",
      "Epoch 110/300\n",
      "43/43 [==============================] - 23s 529ms/step - loss: 0.2818\n",
      "Epoch 111/300\n",
      "43/43 [==============================] - 23s 530ms/step - loss: 0.2819\n",
      "Epoch 112/300\n",
      "43/43 [==============================] - 23s 529ms/step - loss: 0.2858\n",
      "Epoch 113/300\n",
      "43/43 [==============================] - 23s 530ms/step - loss: 0.2881\n",
      "Epoch 114/300\n",
      "43/43 [==============================] - 23s 529ms/step - loss: 0.2854\n",
      "Epoch 115/300\n",
      "43/43 [==============================] - 23s 528ms/step - loss: 0.2858\n",
      "Epoch 116/300\n",
      "43/43 [==============================] - 23s 528ms/step - loss: 0.2857\n",
      "Epoch 117/300\n",
      "43/43 [==============================] - 23s 528ms/step - loss: 0.2882\n",
      "Epoch 118/300\n",
      "43/43 [==============================] - 23s 530ms/step - loss: 0.2882\n",
      "Epoch 119/300\n",
      "43/43 [==============================] - 23s 529ms/step - loss: 0.2858\n",
      "Epoch 120/300\n",
      "43/43 [==============================] - 23s 529ms/step - loss: 0.2840\n",
      "Epoch 121/300\n",
      "43/43 [==============================] - 23s 528ms/step - loss: 0.2790\n",
      "Epoch 122/300\n",
      "43/43 [==============================] - 23s 530ms/step - loss: 0.2738\n",
      "Epoch 123/300\n",
      "43/43 [==============================] - 23s 528ms/step - loss: 0.2711\n",
      "Epoch 124/300\n",
      "43/43 [==============================] - 23s 531ms/step - loss: 0.2663\n",
      "Epoch 125/300\n",
      "43/43 [==============================] - 23s 530ms/step - loss: 0.3237\n",
      "Epoch 126/300\n",
      "43/43 [==============================] - 23s 531ms/step - loss: 0.3405\n",
      "Epoch 127/300\n",
      "43/43 [==============================] - 23s 532ms/step - loss: 0.3364\n",
      "Epoch 128/300\n",
      "43/43 [==============================] - 23s 528ms/step - loss: 0.3200\n",
      "Epoch 129/300\n",
      "43/43 [==============================] - 23s 531ms/step - loss: 0.2984\n",
      "Epoch 130/300\n",
      "43/43 [==============================] - 23s 532ms/step - loss: 0.2783\n",
      "Epoch 131/300\n",
      "43/43 [==============================] - 23s 531ms/step - loss: 0.2643\n",
      "Epoch 132/300\n",
      "43/43 [==============================] - 23s 531ms/step - loss: 0.2534\n",
      "Epoch 133/300\n",
      "43/43 [==============================] - 23s 529ms/step - loss: 0.2431\n",
      "Epoch 134/300\n",
      "43/43 [==============================] - 23s 530ms/step - loss: 0.2365\n",
      "Epoch 135/300\n",
      "43/43 [==============================] - 23s 530ms/step - loss: 0.2342\n",
      "Epoch 136/300\n",
      "43/43 [==============================] - 23s 531ms/step - loss: 0.2310\n",
      "Epoch 137/300\n",
      "43/43 [==============================] - 23s 529ms/step - loss: 0.2290\n",
      "Epoch 138/300\n",
      "43/43 [==============================] - 23s 531ms/step - loss: 0.2279\n",
      "Epoch 139/300\n",
      "43/43 [==============================] - 23s 532ms/step - loss: 0.2288\n",
      "Epoch 140/300\n",
      "43/43 [==============================] - 23s 530ms/step - loss: 0.2337\n",
      "Epoch 141/300\n",
      "43/43 [==============================] - 23s 531ms/step - loss: 0.2389\n",
      "Epoch 142/300\n",
      "43/43 [==============================] - 23s 532ms/step - loss: 0.2483\n",
      "Epoch 143/300\n",
      "43/43 [==============================] - 23s 530ms/step - loss: 0.2666\n",
      "Epoch 144/300\n",
      "43/43 [==============================] - 23s 530ms/step - loss: 0.2876\n",
      "Epoch 145/300\n",
      "43/43 [==============================] - 23s 529ms/step - loss: 0.3246\n",
      "Epoch 146/300\n",
      "43/43 [==============================] - 23s 531ms/step - loss: 0.3538\n",
      "Epoch 147/300\n",
      "43/43 [==============================] - 23s 530ms/step - loss: 0.3566\n",
      "Epoch 148/300\n",
      "43/43 [==============================] - 23s 529ms/step - loss: 0.3418\n",
      "Epoch 149/300\n",
      "43/43 [==============================] - 23s 530ms/step - loss: 0.3143\n",
      "Epoch 150/300\n",
      "43/43 [==============================] - 23s 531ms/step - loss: 0.2898\n",
      "Epoch 151/300\n",
      "43/43 [==============================] - 23s 530ms/step - loss: 0.2677\n",
      "Epoch 152/300\n",
      "43/43 [==============================] - 23s 530ms/step - loss: 0.2502\n",
      "Epoch 153/300\n",
      "43/43 [==============================] - 23s 529ms/step - loss: 0.2374\n",
      "Epoch 154/300\n",
      "43/43 [==============================] - 23s 529ms/step - loss: 0.2272\n",
      "Epoch 155/300\n",
      "43/43 [==============================] - 23s 529ms/step - loss: 0.2230\n",
      "Epoch 156/300\n",
      "43/43 [==============================] - 23s 531ms/step - loss: 0.2204\n",
      "Epoch 157/300\n",
      "43/43 [==============================] - 23s 531ms/step - loss: 0.2188\n",
      "Epoch 158/300\n",
      "43/43 [==============================] - 23s 531ms/step - loss: 0.2189\n",
      "Epoch 159/300\n",
      "43/43 [==============================] - 23s 530ms/step - loss: 0.2184\n",
      "Epoch 160/300\n",
      "43/43 [==============================] - 23s 532ms/step - loss: 0.2166\n",
      "Epoch 161/300\n",
      "43/43 [==============================] - 23s 531ms/step - loss: 0.2190\n",
      "Epoch 162/300\n",
      "43/43 [==============================] - 23s 532ms/step - loss: 0.2220\n",
      "Epoch 163/300\n",
      "43/43 [==============================] - 23s 530ms/step - loss: 0.2245\n",
      "Epoch 164/300\n",
      "43/43 [==============================] - 23s 530ms/step - loss: 0.2303\n",
      "Epoch 165/300\n",
      "43/43 [==============================] - 23s 529ms/step - loss: 0.2419\n",
      "Epoch 166/300\n",
      "43/43 [==============================] - 23s 531ms/step - loss: 0.2564\n",
      "Epoch 167/300\n",
      "43/43 [==============================] - 23s 529ms/step - loss: 0.2736\n",
      "Epoch 168/300\n",
      "43/43 [==============================] - 23s 530ms/step - loss: 0.3078\n",
      "Epoch 169/300\n",
      "43/43 [==============================] - 23s 529ms/step - loss: 0.3355\n",
      "Epoch 170/300\n",
      "43/43 [==============================] - 23s 529ms/step - loss: 0.3543\n",
      "Epoch 171/300\n",
      "43/43 [==============================] - 23s 528ms/step - loss: 0.3533\n",
      "Epoch 172/300\n",
      "43/43 [==============================] - 23s 530ms/step - loss: 0.3276\n",
      "Epoch 173/300\n",
      "43/43 [==============================] - 23s 531ms/step - loss: 0.2987\n",
      "Epoch 174/300\n",
      "43/43 [==============================] - 23s 531ms/step - loss: 0.2711\n",
      "Epoch 175/300\n",
      "43/43 [==============================] - 23s 531ms/step - loss: 0.2505\n",
      "Epoch 176/300\n",
      "43/43 [==============================] - 23s 530ms/step - loss: 0.2342\n",
      "Epoch 177/300\n",
      "43/43 [==============================] - 23s 529ms/step - loss: 0.2234\n",
      "Epoch 178/300\n",
      "43/43 [==============================] - 23s 530ms/step - loss: 0.2178\n",
      "Epoch 179/300\n",
      "43/43 [==============================] - 23s 528ms/step - loss: 0.2102\n",
      "Epoch 180/300\n",
      "43/43 [==============================] - 23s 529ms/step - loss: 0.2079\n",
      "Epoch 181/300\n",
      "43/43 [==============================] - 23s 530ms/step - loss: 0.2072\n",
      "Epoch 182/300\n",
      "43/43 [==============================] - 23s 531ms/step - loss: 0.2054\n",
      "Epoch 183/300\n",
      "43/43 [==============================] - 23s 531ms/step - loss: 0.2050\n",
      "Epoch 184/300\n",
      "43/43 [==============================] - 23s 533ms/step - loss: 0.2045\n",
      "Epoch 185/300\n",
      "43/43 [==============================] - 23s 529ms/step - loss: 0.2053\n",
      "Epoch 186/300\n",
      "43/43 [==============================] - 23s 529ms/step - loss: 0.2082\n",
      "Epoch 187/300\n",
      "43/43 [==============================] - 23s 529ms/step - loss: 0.2091\n",
      "Epoch 188/300\n",
      "43/43 [==============================] - 23s 529ms/step - loss: 0.2143\n",
      "Epoch 189/300\n",
      "43/43 [==============================] - 23s 530ms/step - loss: 0.2211\n",
      "Epoch 190/300\n",
      "43/43 [==============================] - 23s 530ms/step - loss: 0.2322\n",
      "Epoch 191/300\n",
      "43/43 [==============================] - 23s 529ms/step - loss: 0.2450\n",
      "Epoch 192/300\n",
      "43/43 [==============================] - 23s 529ms/step - loss: 0.2684\n",
      "Epoch 193/300\n",
      "43/43 [==============================] - 23s 530ms/step - loss: 0.3066\n",
      "Epoch 194/300\n",
      "43/43 [==============================] - 23s 530ms/step - loss: 0.3441\n",
      "Epoch 195/300\n",
      "43/43 [==============================] - 23s 529ms/step - loss: 0.3755\n",
      "Epoch 196/300\n",
      "43/43 [==============================] - 23s 529ms/step - loss: 0.3714\n",
      "Epoch 197/300\n",
      "43/43 [==============================] - 23s 529ms/step - loss: 0.3469\n",
      "Epoch 198/300\n",
      "43/43 [==============================] - 23s 528ms/step - loss: 0.3150\n",
      "Epoch 199/300\n",
      "43/43 [==============================] - 23s 528ms/step - loss: 0.2868\n",
      "Epoch 200/300\n",
      "43/43 [==============================] - 23s 528ms/step - loss: 0.2652\n",
      "Epoch 201/300\n",
      "43/43 [==============================] - 23s 530ms/step - loss: 0.2411\n",
      "Epoch 202/300\n",
      "43/43 [==============================] - 23s 529ms/step - loss: 0.2229\n",
      "Epoch 203/300\n",
      "43/43 [==============================] - 23s 529ms/step - loss: 0.2141\n",
      "Epoch 204/300\n",
      "43/43 [==============================] - 23s 531ms/step - loss: 0.2058\n",
      "Epoch 205/300\n",
      "43/43 [==============================] - 23s 529ms/step - loss: 0.2016\n",
      "Epoch 206/300\n",
      "43/43 [==============================] - 23s 531ms/step - loss: 0.1989\n",
      "Epoch 207/300\n",
      "43/43 [==============================] - 23s 531ms/step - loss: 0.1982\n",
      "Epoch 208/300\n",
      "43/43 [==============================] - 23s 532ms/step - loss: 0.1962\n",
      "Epoch 209/300\n",
      "43/43 [==============================] - 23s 530ms/step - loss: 0.1978\n",
      "Epoch 210/300\n",
      "43/43 [==============================] - 23s 533ms/step - loss: 0.1966\n",
      "Epoch 211/300\n",
      "43/43 [==============================] - 23s 530ms/step - loss: 0.1967\n",
      "Epoch 212/300\n",
      "43/43 [==============================] - 23s 531ms/step - loss: 0.1968\n",
      "Epoch 213/300\n",
      "43/43 [==============================] - 23s 529ms/step - loss: 0.1978\n",
      "Epoch 214/300\n",
      "43/43 [==============================] - 23s 530ms/step - loss: 0.2001\n",
      "Epoch 215/300\n",
      "43/43 [==============================] - 23s 530ms/step - loss: 0.2016\n",
      "Epoch 216/300\n",
      "43/43 [==============================] - 23s 532ms/step - loss: 0.2097\n",
      "Epoch 217/300\n",
      "43/43 [==============================] - 23s 531ms/step - loss: 0.2269\n",
      "Epoch 218/300\n",
      "43/43 [==============================] - 23s 532ms/step - loss: 0.2535\n",
      "Epoch 219/300\n",
      "43/43 [==============================] - 23s 529ms/step - loss: 0.3076\n",
      "Epoch 220/300\n",
      "43/43 [==============================] - 23s 529ms/step - loss: 0.3841\n",
      "Epoch 221/300\n",
      "43/43 [==============================] - 23s 531ms/step - loss: 0.4266\n",
      "Epoch 222/300\n",
      "43/43 [==============================] - 23s 530ms/step - loss: 0.4063\n",
      "Epoch 223/300\n",
      "43/43 [==============================] - 23s 530ms/step - loss: 0.3574\n",
      "Epoch 224/300\n",
      "43/43 [==============================] - 23s 529ms/step - loss: 0.3099\n",
      "Epoch 225/300\n",
      "43/43 [==============================] - 23s 529ms/step - loss: 0.2693\n",
      "Epoch 226/300\n",
      "43/43 [==============================] - 23s 529ms/step - loss: 0.2423\n",
      "Epoch 227/300\n",
      "43/43 [==============================] - 23s 529ms/step - loss: 0.2263\n",
      "Epoch 228/300\n",
      "43/43 [==============================] - 23s 529ms/step - loss: 0.2131\n",
      "Epoch 229/300\n",
      "43/43 [==============================] - 23s 531ms/step - loss: 0.2031\n",
      "Epoch 230/300\n",
      "43/43 [==============================] - 23s 529ms/step - loss: 0.1977\n",
      "Epoch 231/300\n",
      "43/43 [==============================] - 23s 528ms/step - loss: 0.1997\n",
      "Epoch 232/300\n",
      "43/43 [==============================] - 23s 528ms/step - loss: 0.1962\n",
      "Epoch 233/300\n",
      "43/43 [==============================] - 23s 528ms/step - loss: 0.1939\n",
      "Epoch 234/300\n",
      "43/43 [==============================] - 23s 529ms/step - loss: 0.1916\n",
      "Epoch 235/300\n",
      "43/43 [==============================] - 23s 530ms/step - loss: 0.2081\n",
      "Epoch 236/300\n",
      "43/43 [==============================] - 23s 531ms/step - loss: 0.2024\n",
      "Epoch 237/300\n",
      "43/43 [==============================] - 23s 531ms/step - loss: 0.2009\n",
      "Epoch 238/300\n",
      "43/43 [==============================] - 23s 531ms/step - loss: 0.1978\n",
      "Epoch 239/300\n",
      "43/43 [==============================] - 23s 528ms/step - loss: 0.1951\n",
      "Epoch 240/300\n",
      "43/43 [==============================] - 23s 530ms/step - loss: 0.1947\n",
      "Epoch 241/300\n",
      "43/43 [==============================] - 23s 530ms/step - loss: 0.1925\n",
      "Epoch 242/300\n",
      "43/43 [==============================] - 23s 528ms/step - loss: 0.1930\n",
      "Epoch 243/300\n",
      "43/43 [==============================] - 23s 530ms/step - loss: 0.1937\n",
      "Epoch 244/300\n",
      "43/43 [==============================] - 23s 528ms/step - loss: 0.1981\n",
      "Epoch 245/300\n",
      "43/43 [==============================] - 23s 530ms/step - loss: 0.1990\n",
      "Epoch 246/300\n",
      "43/43 [==============================] - 23s 529ms/step - loss: 0.2071\n",
      "Epoch 247/300\n",
      "43/43 [==============================] - 23s 529ms/step - loss: 0.2185\n",
      "Epoch 248/300\n",
      "43/43 [==============================] - 23s 530ms/step - loss: 0.2371\n",
      "Epoch 249/300\n",
      "43/43 [==============================] - 23s 529ms/step - loss: 0.2666\n",
      "Epoch 250/300\n",
      "43/43 [==============================] - 23s 528ms/step - loss: 0.3102\n",
      "Epoch 251/300\n",
      "43/43 [==============================] - 23s 528ms/step - loss: 0.3694\n",
      "Epoch 252/300\n",
      "43/43 [==============================] - 23s 529ms/step - loss: 0.4043\n",
      "Epoch 253/300\n",
      "43/43 [==============================] - 23s 528ms/step - loss: 0.3935\n",
      "Epoch 254/300\n",
      "43/43 [==============================] - 23s 529ms/step - loss: 0.3549\n",
      "Epoch 255/300\n",
      "43/43 [==============================] - 23s 529ms/step - loss: 0.3108\n",
      "Epoch 256/300\n",
      "43/43 [==============================] - 23s 528ms/step - loss: 0.2764\n",
      "Epoch 257/300\n",
      "43/43 [==============================] - 23s 528ms/step - loss: 0.2460\n",
      "Epoch 258/300\n",
      "43/43 [==============================] - 23s 529ms/step - loss: 0.2239\n",
      "Epoch 259/300\n",
      "43/43 [==============================] - 23s 528ms/step - loss: 0.2083\n",
      "Epoch 260/300\n",
      "43/43 [==============================] - 23s 528ms/step - loss: 0.1987\n",
      "Epoch 261/300\n",
      "43/43 [==============================] - 23s 528ms/step - loss: 0.1944\n",
      "Epoch 262/300\n",
      "43/43 [==============================] - 23s 529ms/step - loss: 0.1903\n",
      "Epoch 263/300\n",
      "43/43 [==============================] - 23s 527ms/step - loss: 0.1875\n",
      "Epoch 264/300\n",
      "43/43 [==============================] - 23s 529ms/step - loss: 0.1875\n",
      "Epoch 265/300\n",
      "43/43 [==============================] - 23s 530ms/step - loss: 0.1846\n",
      "Epoch 266/300\n",
      "43/43 [==============================] - 23s 528ms/step - loss: 0.1824\n",
      "Epoch 267/300\n",
      "43/43 [==============================] - 23s 528ms/step - loss: 0.1818\n",
      "Epoch 268/300\n",
      "43/43 [==============================] - 23s 528ms/step - loss: 0.1831\n",
      "Epoch 269/300\n",
      "43/43 [==============================] - 23s 526ms/step - loss: 0.1858\n",
      "Epoch 270/300\n",
      "43/43 [==============================] - 23s 526ms/step - loss: 0.1884\n",
      "Epoch 271/300\n",
      "43/43 [==============================] - 23s 528ms/step - loss: 0.1882\n",
      "Epoch 272/300\n",
      "43/43 [==============================] - 23s 528ms/step - loss: 0.1890\n",
      "Epoch 273/300\n",
      "43/43 [==============================] - 23s 528ms/step - loss: 0.1930\n",
      "Epoch 274/300\n",
      "43/43 [==============================] - 23s 527ms/step - loss: 0.1947\n",
      "Epoch 275/300\n",
      "43/43 [==============================] - 23s 528ms/step - loss: 0.1977\n",
      "Epoch 276/300\n",
      "43/43 [==============================] - 23s 530ms/step - loss: 0.2015\n",
      "Epoch 277/300\n",
      "43/43 [==============================] - 23s 528ms/step - loss: 0.2159\n",
      "Epoch 278/300\n",
      "43/43 [==============================] - 23s 530ms/step - loss: 0.2309\n",
      "Epoch 279/300\n",
      "43/43 [==============================] - 23s 529ms/step - loss: 0.2457\n",
      "Epoch 280/300\n",
      "43/43 [==============================] - 23s 531ms/step - loss: 0.2834\n",
      "Epoch 281/300\n",
      "43/43 [==============================] - 23s 529ms/step - loss: 0.3365\n",
      "Epoch 282/300\n",
      "43/43 [==============================] - 23s 527ms/step - loss: 0.3688\n",
      "Epoch 283/300\n",
      "43/43 [==============================] - 23s 531ms/step - loss: 0.3743\n",
      "Epoch 284/300\n",
      "43/43 [==============================] - 23s 529ms/step - loss: 0.3604\n",
      "Epoch 285/300\n",
      "43/43 [==============================] - 23s 530ms/step - loss: 0.3184\n",
      "Epoch 286/300\n",
      "43/43 [==============================] - 23s 531ms/step - loss: 0.2808\n",
      "Epoch 287/300\n",
      "43/43 [==============================] - 23s 530ms/step - loss: 0.2496\n",
      "Epoch 288/300\n",
      "43/43 [==============================] - 23s 532ms/step - loss: 0.2249\n",
      "Epoch 289/300\n",
      "43/43 [==============================] - 23s 532ms/step - loss: 0.2093\n",
      "Epoch 290/300\n",
      "43/43 [==============================] - 23s 529ms/step - loss: 0.1978\n",
      "Epoch 291/300\n",
      "43/43 [==============================] - 23s 529ms/step - loss: 0.1909\n",
      "Epoch 292/300\n",
      "43/43 [==============================] - 23s 529ms/step - loss: 0.1853\n",
      "Epoch 293/300\n",
      "43/43 [==============================] - 23s 528ms/step - loss: 0.1836\n",
      "Epoch 294/300\n",
      "43/43 [==============================] - 23s 530ms/step - loss: 0.1818\n",
      "Epoch 295/300\n",
      "43/43 [==============================] - 23s 529ms/step - loss: 0.1825\n",
      "Epoch 296/300\n",
      "43/43 [==============================] - 23s 529ms/step - loss: 0.1843\n",
      "Epoch 297/300\n",
      "43/43 [==============================] - 23s 530ms/step - loss: 0.1892\n",
      "Epoch 298/300\n",
      "43/43 [==============================] - 23s 530ms/step - loss: 0.1900\n",
      "Epoch 299/300\n",
      "43/43 [==============================] - 23s 530ms/step - loss: 0.1949\n",
      "Epoch 300/300\n",
      "43/43 [==============================] - 23s 530ms/step - loss: 0.1937\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(dataset.repeat(), epochs=EPOCHS, steps_per_epoch=steps_per_epoch,\n",
    "                    callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "gather": {
     "logged": 1607290499474
    },
    "id": "Llxn7wf-fXol",
    "outputId": "907b863f-064e-40bf-8d42-ef6c243e08c0"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'./training_checkpoints/ckpt_300'"
      ]
     },
     "execution_count": 27,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.latest_checkpoint(checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1607290498758
    },
    "id": "e_z9F0GIfXol"
   },
   "outputs": [],
   "source": [
    "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
    "\n",
    "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "\n",
    "model.build(tf.TensorShape([1, None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "gather": {
     "logged": 1607290508727
    },
    "id": "VCVhV3BQfXom",
    "outputId": "410a2647-b03c-467a-b96e-c3d424a63409"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (1, None, 256)            11776     \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (1, None, 1024)           5246976   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (1, None, 46)             47150     \n",
      "=================================================================\n",
      "Total params: 5,305,902\n",
      "Trainable params: 5,305,902\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1607296269727
    },
    "id": "n1LXYzZ2fXom"
   },
   "outputs": [],
   "source": [
    "\n",
    "def generate_text(model, start_string):\n",
    "  # Evaluation step (generating text using the learned model)\n",
    "  global candidates_text\n",
    "  # Number of characters to generate\n",
    "  num_generate = 400 \n",
    "\n",
    "  # Converting our start string to numbers (vectorizing)\n",
    "  input_eval = [char2idx[s] for s in start_string]\n",
    "  input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "  # Empty string to store our results\n",
    "  text_generated = []\n",
    "\n",
    "  # Low temperatures results in more predictable text.\n",
    "  # Higher temperatures results in more surprising text.\n",
    "  # Experiment to find the best setting.\n",
    "  temperature = 1.0 \n",
    "  # Here batch size == 1\n",
    "  model.reset_states()\n",
    "  for i in range(num_generate):\n",
    "      predictions = model(input_eval)\n",
    "      # remove the batch dimension\n",
    "      predictions = tf.squeeze(predictions, 0)\n",
    "\n",
    "      # using a multinomial distribution to predict the word returned by the model\n",
    "      predictions = predictions / temperature\n",
    "\n",
    "      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "      #print(tf.multinomial(predictions, num_samples=1).shape)\n",
    "      # We pass the predicted word as the next input to the model\n",
    "      # along with the previous hidden state\n",
    "      input_eval = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "      text_generated.append(idx2char[predicted_id])\n",
    "  candidates_text=start_string + ''.join(text_generated)\n",
    "  return (start_string + ''.join(text_generated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "gather": {
     "logged": 1607296360732
    },
    "id": "HWdjpvAEfXom",
    "outputId": "6e426570-9d70-4bea-d243-814faa2ada4c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "كم تَطْلُبُونَ لَنَا عَيْباً فيُعجِزُكمْ وذوْلُوا\n",
      "إِلى ذا الحُسامُ عَلى الرِسالِ بِهِ\n",
      "مِن كاسِ شَرَّ الواحُ مِن طَيرِ أَجرَدِ\n",
      "رَعى مَردىً لَم يَخلُ مِنهُ ماكَ مُبدِعِ\n",
      "أَسالَت سَلاماً وَلا بَلَغتُ شِعالبي أُمَّ القُرون\n",
      " الرومُ ماذِلُ عِلمِ قُروبٍ دارِ\n",
      "\n",
      "وَمِن عَجيبٍ لِغَيرِ الهلم مُشتَسيا طالِباً\n",
      "شَغَفَ الإِصليقُ نَقَّكَ واِعجَدى\n",
      "مُزهَر مَدَّت حصالَ للمجدِ بُرْعَه\n",
      "يُبْدَى نُجومُ العاذَنْ إذا ما عجا\n",
      "فَتَرى لَهُم فيه الطَّرَا الْعَزْمُ مِنّ\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(model, start_string=\"كم تَطْلُبُونَ لَنَا عَيْباً فيُعجِزُكمْ\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "6n9ooVKBfXom",
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    },
    "outputId": "a79f49bd-2830-48ee-84f3-c65453036ca4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "وَأَحسَنُ مِنكَ لَم تَرَ قَطُّ عَيني وَأَعظُمُ غُرَّةٍ\n",
      "يُعازِلُ في حَقٍّ اِتَّخَدتَ بِها\n",
      "وَفي العُراقِ إِذا أَضَّلتَ حَبّاً\n",
      "هُوَ المَصرُ لِلدُنيا ضَليلِ الصُبحِ\n",
      "مَعفونَفَينِ بِهِ\n",
      "مَكانَ عَلى أَيّامِن الجَنزِلبِسِ وَهوَ سُوري\n",
      "بِالوَصلُ مِن حُسنِ وَصنِ ذي الإِلَهِ\n",
      "\n",
      "يا أَيُّها النَفسُ ابَةُ الغَيَّ يَقْفى الْمُصَفَّرَه\n",
      "وَلدَّلِي أُوُوسٍ\n",
      "عَلَيْهِ ابنهُ مالُهُ في حُجَّةٍ مِنَ النَدى\n",
      "ذَريعٌ كَما غَدَّ الغَمامُ عَلى الدُجى\n",
      "سَمى وَجفَكرُها \n"
     ]
    }
   ],
   "source": [
    "print(generate_text(model, start_string=\"وَأَحسَنُ مِنكَ لَم تَرَ قَطُّ عَيني\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "djSvP1A8fXom",
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    },
    "outputId": "5c7ede2a-822c-4f14-ea21-c3dc290c24cb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "تَصغُرُ في عَينِ العَظيمِ العَظائِمُ\n",
      "وَما بَينَ مُنتَمِعٍ عِندَ نَعماكَ\n",
      " إِذ يَرجَعُ سُلطانَ حاجِبِ\n",
      "وَقَد جاءَ عَن كُلِّ الهَوى في إِجلالِهِ\n",
      "وَكَأَنَّ عادَ الخَليفَةِ ن كُنتَ قَولاً\n",
      "لَم تَضرِ يَدنو الَّذي تَلهَب مَنازِلَها\n",
      "بِالعيدِ وَالدُنيا إِلى اللَهوِ الُ\n",
      "ناجٍ منهمُ وتعدرُ الشي\n",
      "رَ ولا من تَحْدهُدَ مَشْرى الهُداما\n",
      "وَمَن كُنتُ كَالمُقَفصيد كَأَنَّها أَحواهُ\n",
      "بُقُيَةُ اللَّبِذينَ تَمَسُّغُه\n",
      "دُ ظُلَّةٌ تُمَدّى بِها\n",
      "وَذَناءُ الحَشاوِلِ\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(model, start_string=\"تَصغُرُ في عَينِ العَظيمِ العَظائِمُ\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XEtEFdh9fXon",
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    },
    "outputId": "a8ef9e43-acb6-4386-8e9d-2324fcbdc626"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "الخيلُ والليلُ والبَيداءُ تعرِفُني\n",
      "مُتَنَجَّةٌ بَينَ ذَيّالمولَها\n",
      "إِذا اِستَحسَنَت بِالباعِ با رامَ الأَمرَ كالخَخرِ\n",
      " بِها يَعدو شَفاعَيها يَجومُ\n",
      "العُلا السَّبقْ\n",
      "خَفِيعتَ من أسقيرِهِمْ وخُزْلاً\n",
      "تَمُنَّ الأرْضُ من نهجِزامَ إذا الحق اندثر\n",
      "لا الذي يحرقُ امجاداً لهُ\n",
      "أسعدَتهُ مثلَ حُلمٍ يُحتقر\n",
      "أتى اغتزامُ\n",
      "عَهدُ اللَهِ لَم تُفقِن إِلى وَجهِ جَمعِ المَوكِبِ\n",
      "حَولى وَحَقِّقُهُ أَضنى مَقامُ\n",
      "مزَى عَنْهُ أَثْوَابَ الْفَنَاءِ وَرَفْرَفَتْ\n",
      "إِلَ\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(model, start_string=\"الخيلُ والليلُ والبَيداءُ تعرِفُني\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZipgMnc3Qr7a",
    "outputId": "bf5e193f-04f3-4689-94c3-264444e0a3d3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "تجاوزت مقدار الشجاعة والنهى\n",
      "تفّاء التي تُحظي بوان\n",
      "هذي عبدٌ للشرس موساً\n",
      "يضيعا أرواحُ كأني مُذْعِبٌ\n",
      "لين ضؤوف كف شَاءَ بَعدَ تَفَرُّقٍ\n",
      "ما لِلِقاءِ وَلِلفُراقِ دَوامُ\n",
      "سَيَشُدُّ أَزرَكَ وَالشَدائِدُ جُمعَكالٌ يَحتَسي\n",
      "لِغَيرِكَ إِن أَطيقٌ فَيُتلَقَظُ\n",
      "أَنا لِلأَغَرِّ مُخَلِّدينَ بِها\n",
      "جِنّي بِلادٌ تُنتَ فَاِلفَقِدَت\n",
      "بِهِ أَو قالونِ مَأْتُهُمْ\n",
      "فَلَمَّا أَتَيْنَا كَريمِهِ وَالرَشاها غَيرَ مُرتائِهِ\n",
      "وَكِلا بِقَرآٍ مِثلَ خَلقِ اللَولِ \n"
     ]
    }
   ],
   "source": [
    "print(generate_text(model, start_string=\"تجاوزت مقدار الشجاعة والنهى\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "zGew0yiUfXon",
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "!pip install -q nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "gather": {
     "logged": 1607449335817
    },
    "id": "AdiYV08afXon",
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "import io\n",
    "# Read, then decode for py2 compat.\n",
    "with io. open('/content/sample_data/praise-poems_dataset.txt', 'r',encoding='utf8')as f:\n",
    "   text = f.read()\n",
    "# remove some exteranous chars \n",
    "execluded = '!()/*-.1:=[]«»;؛−,،~?؟#\\u200f\\ufeff'\n",
    "#execluded2 ='\"'\n",
    "#execluded3 =\"'\"\n",
    "out = \"\"\n",
    "\n",
    "for char in text:\n",
    "  if char not in (execluded):\n",
    "    out += char\n",
    "out = out.replace(\"\\t\\t\\t\", \"\\t\")\n",
    "out = out.replace(\"\\r\\r\\n\", \"\\n\")\n",
    "out = out.replace(\"\\r\\n\",\"\\n\")\n",
    "out = out.replace(\"\\t\\n\", \"\\n\")\n",
    "out = out.replace(\"\\n\\n\", \"\\n\")\n",
    "out = out.replace('\"', \"\")\n",
    "out = out.replace(\"'\", \"\")\n",
    "\n",
    "# process Unicode text\n",
    "with io.open('/content/sample_data/Cleandatabase.txt', 'w', encoding='utf8') as f:\n",
    "    f.write(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "gather": {
     "logged": 1607296396971
    },
    "id": "KwFBf-f1fXon",
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    },
    "outputId": "05316aee-5047-453a-fe3a-3308657568e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34462\n"
     ]
    }
   ],
   "source": [
    "reference_text= open('/content/sample_data/Cleandatabase.txt', 'rb').read().decode(encoding='utf-8')\n",
    "\n",
    "verse_Lines=reference_text.splitlines()\n",
    "references=list()\n",
    "for i in range(len(verse_Lines)):\n",
    "    s=(verse_Lines[i].split())\n",
    "    if s :\n",
    "     references.append(s)\n",
    "    \n",
    "print(len(references))\n",
    "                       \n",
    "#----------------------------------------------------------------------\n",
    "#print(type(candidates_text))\n",
    "new_verse_Lines=candidates_text.splitlines()\n",
    "candidates=list()\n",
    "for i in range(len(new_verse_Lines)):\n",
    "    s=(new_verse_Lines[i].split())\n",
    "    if s :\n",
    "     candidates.extend(s)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H84CMaMqfXon",
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    },
    "outputId": "01479c8c-77f2-4fd4-bdc9-000973c860ea"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 4-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU scores-Individual 1-gram: 0.600000\n",
      "BLEU scores-Individual 2-gram: 0.109375\n",
      "BLEU scores-Individual 3-gram: 0.031746\n",
      "BLEU scores-Individual 4-gram: 1.000000\n"
     ]
    }
   ],
   "source": [
    "# n-gram individual BLEU\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "print('BLEU scores-Individual 1-gram: %f' % sentence_bleu(references, candidates, weights=(1, 0, 0, 0)))\n",
    "print('BLEU scores-Individual 2-gram: %f' % sentence_bleu(references, candidates, weights=(0, 1, 0, 0)))\n",
    "print('BLEU scores-Individual 3-gram: %f' % sentence_bleu(references, candidates, weights=(0, 0, 1, 0)))\n",
    "print('BLEU scores-Individual 4-gram: %f' % sentence_bleu(references, candidates, weights=(0, 0, 0, 1)))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "LSTM-Poems-Generator.ipynb",
   "provenance": []
  },
  "kernel_info": {
   "name": "python3-azureml"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
